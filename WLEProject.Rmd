---
title: "Weight Lifting Exercises Performance Prediction"
output:
  word_document: default
  pdf_document: default
  html_document:
    keep_md: yes
---
## Executive Summary

In this report Weight Lifting Exercises dataset is used to train machine learning algorithms to predict how well activities were performed. The dataset contains data from sensors in the users' glove, armband, lumbar belt and dumbbell, participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways, these sensor data are then used to predict their performance. 

First, the dataset is explored, processed to transform into a tidy dataset to minimize runtime, then machine learning algorithms are built for prediction. Since the process is very time consuming, extra efforts are put into removing unnecessary data to reduce runtime.

Generalized Boosted Regression Models (gbm) and Random Forest (rf) are trained and tested to select the best one. Random Forest, selected for offering  higher accuracy, is cross validated and then tested for out of sample errors.

Finally, the Random Forest model is applied to SubmitTest data stored in pml-testing.csv to predict the performance of those 20 observasions, the results are saved in 20 files to be submitted for this project.

## Data Processing

```{r, echo = FALSE, message = FALSE}
# Set working directory 
# setwd("C:\\z_DATAJHU2015\\PracticalMachineLearning201502\\Project")
library(knitr)
opts_chunk$set(echo = TRUE, message = FALSE, cache = TRUE)
require(ggplot2) 
require(caret)
require(parallel)
require(randomForest)
require(gbm)
```

### Load data


```{r}

# Get data
training <- read.csv("pml-training.csv", header = TRUE)
SubmitTest  <- read.csv('pml-testing.csv')
dim(training)
```
The dataset contains a lot of unnecessary data which should be removed to reduce processing time. 

### Clean data 

The above data from pml-training.csv are preprocessed to minimize the number of variables but still capture a large amount of variability in the following steps:

```{r}
# Remove columns with over a 90% of NAs
NAPerColumn <- apply(training,2,function(x) {sum(is.na(x))});
training <- training[,which(NAPerColumn <  nrow(training)*0.9)];  
# Remove near zero variance predictors
NearZeroColumns <- nearZeroVar(training, saveMetrics = TRUE)
training <- training[, NearZeroColumns$nzv==FALSE]
# Remove irrelevant columns
training<-training[,7:ncol(training)]
# Convert classe into factor
training$classe <- factor(training$classe)
```

### Partition Data

The processed data are split into three partitions:

- TrainData 60% for training, creating the model 

- TestData 20% for testing and cross validation

- OOSData 20% for testing out of sample errors

```{r}

TrainIndex <- createDataPartition(y = training$classe, p=0.6,list=FALSE);
TrainData <- training[TrainIndex, ];
TestOOSData <- training[-TrainIndex, ];

TestOOSIndex <- createDataPartition(y = TestOOSData$classe, p=0.5,list=FALSE);
TestData <- TestOOSData[TestOOSIndex, ];
OOSData <- TestOOSData[-TestOOSIndex, ];
dim(TestData)
dim(OOSData)
```

## Create machine learning models

Generalized Boosted Regression Models (gbm) and Random Forest are trained and tested on variable classe to select the one with highest accuracy. 

### Generalized Boosted Regression Model (gbm)

```{r, eval=FALSE}
set.seed(2)
ModelGBM <-train(classe ~ ., method = 'gbm', data = TrainData)
GBMAccuracy <- predict(ModelGBM , TestData)
print(confusionMatrix(GBMAccuracy, TestData$classe))
```
Generalized Boosted Regression Model (gbm) Overall Statistics
                                          
                Accuracy : 0.9587         
                  95% CI : (0.952, 0.9647)
     No Information Rate : 0.2845         
     P-Value [Acc > NIR] : < 2.2e-16      
                                          
                   Kappa : 0.9477         


### Random Forest Model (rf)

```{r}
set.seed(7)
ModelRF <- randomForest(classe ~ ., data = TrainData, important=TRUE, proximity=TRUE)    
RFAccuracy <- predict(ModelRF, TestData)
print(confusionMatrix(RFAccuracy, TestData$classe))
```

Random Forest, selected for having higher accuracy, is cross validated and then tested for out of sample errors.

### Random Forest Cross Validation

Random Forest Model cross validation is performed 10 times and 10 folds.

```{r}
set.seed(9)
ControlF <- trainControl(method = "repeatedcv", number = 10, repeats = 10)
ModelRFCV <- randomForest(classe ~ ., data = TrainData, important=TRUE, proximity=TRUE, trControl = ControlF)    
RFCVaccuracy <- predict(ModelRFCV, TestData)
print(confusionMatrix(RFCVaccuracy, TestData$classe))
```

### Random Forest Out of Sample Test

Random Forest Model Out of Sample Test is performed on the reserved OOSData which contains 20% of the original data set not used in the model training nor previous test.

```{r}
set.seed(22)
RFOOSAccuracy <- predict(ModelRF, OOSData)
print(confusionMatrix(RFOOSAccuracy, OOSData$classe))
```

## Conclusion

As expected, Random Forest out of sample test shows accuracy slightly lower than the in sample result but still good for prediction.

## Prediction Assignment Submission

The Random Forest model trained and tested above is now applied to SubmitTest data stored in pml-testing.csv to predict the performance of those 20 observasions, the results are saved to 20 files to be submitted for this project.

```{r}

SubmitAnswer <- predict(ModelRF, SubmitTest)

pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(SubmitAnswer)
```






